import { Channels } from './common';
import { OpenAIRequest } from './common';
import { OpenAIResponse } from './common';

import { Configuration } from 'openai';
import { OpenAIApi } from 'openai';

import { ipcMain } from 'electron';

// //////////////////////////////////////////////////////////////////////////
// ðŸŸ© chatCompletion request
// //////////////////////////////////////////////////////////////////////////

// eslint-disable-next-line @typescript-eslint/no-misused-promises
ipcMain.on(Channels.openaiChatCompletion, chatCompletion);

// ðŸ‘‡ exported for tests
export async function chatCompletion(
  event,
  _request: OpenAIRequest
): Promise<OpenAIResponse> {
  // ðŸ‘‡ create the OpenAI client
  const openai = new OpenAIApi(
    new Configuration({
      apiKey: process.env['OPEN_AI_KEY']
    })
  );
  // ðŸ‘‡ these are the request defaults
  const dflt = {
    max_tokens: 2048,
    model: 'gpt-3.5-turbo',
    temperature: 0.5,
    top_p: 1
  };
  // ðŸ‘‡ simplify the API to look the same as "completion"
  const { prompt, ...request } = _request;
  // ðŸ‘‡ ready to call OpenAI
  const response = await openai.createChatCompletion({
    messages: [{ content: prompt, role: 'user' }],
    ...request,
    ...dflt
  });
  // ðŸ‘‡ just extract the important bits
  return {
    finish_reason: response.data.choices[0].finish_reason as any,
    text: response.data.choices[0].message.content
  };
}

// //////////////////////////////////////////////////////////////////////////
// ðŸŸ© completion request
// //////////////////////////////////////////////////////////////////////////

// eslint-disable-next-line @typescript-eslint/no-misused-promises
ipcMain.on(Channels.openaiCompletion, completion);

// ðŸ‘‡ exported for tests
export async function completion(
  event,
  request: OpenAIRequest
): Promise<OpenAIResponse> {
  // ðŸ‘‡ create the OpenAI client
  const openai = new OpenAIApi(
    new Configuration({
      apiKey: process.env['OPEN_AI_KEY']
    })
  );
  // ðŸ‘‡ these are the request defaults
  const dflt = {
    max_tokens: 2048,
    model: 'text-davinci-003',
    temperature: 0.5,
    top_p: 1
  };
  // ðŸ‘‡ ready to call OpenAI
  const response = await openai.createCompletion({
    ...request,
    ...dflt
  });
  // ðŸ‘‡ just extract the important bits
  return {
    finish_reason: response.data.choices[0].finish_reason as any,
    text: response.data.choices[0].text
  };
}

// //////////////////////////////////////////////////////////////////////////
// ðŸŸ© listModels request
// //////////////////////////////////////////////////////////////////////////

// eslint-disable-next-line @typescript-eslint/no-misused-promises
ipcMain.on(Channels.openaiListModels, listModels);

// ðŸ‘‡ exported for tests
export async function listModels(): Promise<string[]> {
  // ðŸ‘‡ create the OpenAI client
  const openai = new OpenAIApi(
    new Configuration({
      apiKey: process.env['OPEN_AI_KEY']
    })
  );
  // ðŸ‘‡ ready to call OpenAI
  const response = await openai.listModels();
  return response.data.data.map((data) => data.id).sort();
}
